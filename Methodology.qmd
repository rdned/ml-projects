---
title: "Methodology"
subtitle: "Analyzing Music and Emotions Through Audience Response"
format: html
toc: true
css: mermaid-fix.css
---

## Methodology

```{mermaid}

%%| fig-cap: "Swimlane Diagram: Multimodal Musicâ€“Emotion Workflow"
flowchart LR

    %% --- Audience Lane ---
    subgraph AUDIENCE["ðŸŽ­ Heatmap Tracking"]
        direction TB
        A1["Track individual heatmaps"]
        A2["Compute heatmap embeddings"]
        A3["Extract denoised emotional signals"]
    end

    %% --- Audio Lane ---
    subgraph AUDIO["ðŸŽµ Audio Embedding"]
        direction TB
        B1["Compute synchronized audio embeddings"]
        B2["Model audio as stochastic process"]
    end

    %% --- Model Lane ---
    subgraph MODEL["ðŸ§  Connecting Music and Emotions"]
        direction TB
        C1["Implement and train ANN"]
        C3["Evaluate causal influence"]
    end

    %% --- XAI Lane ---
    subgraph XAI["âœ¨ Explainable AI Analysis"]
        direction TB
        D1["Apply XAI methods"]
        D2["Identify influential features"]
        D3["Interpret emotional influence"]
    end

    %% --- Crossâ€‘lane flows ---
    A3 --> C1
    B2 --> C1
    C3 --> D1

    %% --- Styling ---
    classDef audienceStyle fill:#e8f4ff,stroke:#0277bd,stroke-width:2px,color:#000
    classDef audioStyle fill:#e8ffe8,stroke:#00796b,stroke-width:2px,color:#000
    classDef modelStyle fill:#fff9d6,stroke:#f57f17,stroke-width:2px,color:#000
    classDef xaiStyle fill:#f7e8ff,stroke:#6a1b9a,stroke-width:2px,color:#000
    classDef nodeStyle fill:#ffffff,stroke:#333,stroke-width:1px,color:#000

    class AUDIENCE audienceStyle
    class AUDIO audioStyle
    class MODEL modelStyle
    class XAI xaiStyle
    class A1,A2,A3,B1,B2,C1,C3,D1,D2,D3 nodeStyle
```

---


### Audience Heatmap Tracking

- Track individual heatmaps corresponding to individual faces in the audience during the opera
- Calculate embeddings for each tracked heatmap representing stochastic processes
- Extract stochastic processes that minimize noise while preserving emotional information related to emotions during listening

### Audio Embedding

- Calculate embeddings of the synchronized audio from the opera
- Model the audio embedding as a stochastic process capturing musical features influencing emotions

### Connecting Music and Emotions

- Implement an Artificial Neural Network (ANN) to connect the stochastic processes from audience emotions and music
- Train the ANN to predict emotions from music features and previous emotional states
- Hypothesize causal influence of music on emotions based on prediction accuracy

### Explainable AI Analysis

- Apply explainable AI methods to the trained ANN
- Identify crucial input features (music or previous emotions) that drive emotion prediction
- Gain insights into how music influences emotional responses

## Expected Outcomes

::: {.callout-note appearance="minimal"}
- A robust model linking musical features to emotional reactions
- Understanding of temporal dynamics of emotions during live music
- Explainable insights into causal relationships between music and emotions
:::

## Future Work

- Extend to other music genres and live settings
- Incorporate physiological data for multimodal emotion analysis
- Refine models with larger datasets and advanced AI techniques